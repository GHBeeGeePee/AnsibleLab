
The Ansible Inventory File 

The Ansible Inventory File defines all the hosts and groups of hosts that you want to automate with Ansible. Also called the Ansible Hosts File it tells Ansible about the hosts that it can connect to, you can also define custom options per host e.g. different port number or credentials

The default Ansible inventory file is located in /etc/ansible/hosts. You can also create project-specific inventory files in alternate locations. 
The inventory file can list individual hosts or user-defined groups of hosts.

For Ansible to automate a Linux Server, Network device or Cloud server it has to exist within the inventory file (also known as the Ansible hosts file) and saved in either YAML or INI format.

The file can also be static or created dynamically by a script.



The default inventory file is named hosts by default and is one of the default files installed with Ansible – the other main one being ansible.cfg. If you cat the file you can see it does not contain any entries (they are all commented out) but what it does do is give you a template of how to start formatting your inventory file.

Ansible Inventory Example
The Ansible hosts file is written in either YAML or INI format. The most common format is INI. Check the ansible/hosts file examples below.

Devices or hosts or nodes are referenced by their DNS name, hostname or with the ansible inventory ip address variable.

host1.domain.com
192.168.1.193  
 
[routers]
router1.domain.com 
router2.domain.com
CSR-1 ansible_host=192.168.1.199
 
[switches]
switch1.domain.com
switch2.domain.com 


Hosts can be defined either by IP address or DNS name or if DNS is not resolvable using the ansible_host command syntax.

In the example above the display name is CSR-1 and the ansible host command specifies the IP address as 192.168.1.199

Hosts can also grouped together by putting under groups named by a name in square brackets e.g. [Routers]
[database servers]
[webservers group]



The file can live anywhere but it’s a best practice to put it in the same directory as your project files.


Ansible Inventory YAML
The hosts file above is written in the default INI format, an Ansible YAML Inventory is show below

ansible inventory file as yaml
Ansible Inventory file Example YAML syntax

In the above example we have the same inventory file but in a YAML format.



Best Practices
Keep your hosts file names simple – I always use hosts
Comment were required to ensure someone else can understand your inventory
Use children to address multiple groups at once
Do no put passwords in your inventory file – reference them in group_vars or host_vars or even better use Ansible vault within those files.


Ansible Hosts File Comments
If you want to put a comment next to any item in your Ansible Hosts file just start a new line with a #



How do I know if my host is reachable in Ansible?
You can use the Ansible ping module to check successful ssh connectivity to target devices – use the command ansible all -m ping -v





Ansible-Playbook Command

ansible-playbook is a command-line tool used to run Ansible Playbooks. A playbook is a collection of instructions in YAML format that define a set of tasks to be executed by Ansible on a group of targeted hosts or machines.

The ansible-playbook command is used to execute a playbook, and it takes the path to the playbook file as an argument. It connects to the targeted hosts specified in the playbook, and runs the tasks defined in the playbook on those hosts.

The basic syntax of the ansible-playbook command is:

ansible-playbook [options] playbook.yml
For example, to run a playbook called playbook.yml, the command would be:

ansible-playbook playbook.yml

Some of the options that can be used with the ansible-playbook command include:

-i or –inventory to specify a custom inventory file
-l or –limit to limit the execution to a specific subset of hosts
-v or –verbose to increase the output verbosity
-e or –extra-vars to pass extra variables to the playbook
-t or –tags to run specific tasks or groups of tasks

–check to check the playbook without making any changes


It’s important to note that the ansible-playbook command must be run on the control machine and it will communicate with the target host over SSH, unless you have configured a different connection method.



Ansible Playbook Elements
Plays – A play is a top level specification of a group of tasks
Tasks – A Task defines which module to use to perform a certain task
Modules – Ansible modules are reusable, standalone scripts that can be used by the Ansible API, or by the ansible or ansible-playbook programs. They return information to ansible by printing a JSON string to stdout before exiting. They take arguments in one of several ways which we’ll go into as we work through this tutorial.
Plugins – Technically Modules are plugins, but a plugin is a piece of code that augment Ansible’s core functionality
Inventory – The Ansible Inventory is a file that contains a list of all the hosts you want to automate. They can put into groups i.e [DB-Servers]
Roles – Roles are re-useable playbooks that you can call on to perform similar tasks


The control machine does not have to be anything special, it can even run on your laptop, it only requires SSH connectivity to the target device. You install Ansible and setup a few configuration files and you are ready to automate!

The basic concepts are:

You define a list of managed nodes in the hosts file

Ansible code is written in a playbook file to describe what tasks to perform on those hosts

You run the playbook

Ansible executes the tasks in order and provides an output of success or fail of the tasks



The first thing you can do with Ansible is to check you have connectivity to your devices. We can issue a ping command that will make sure the inventory is configured correctly and we can ssh to the ip address of each device.

To use the ping module use the following command.

$ ansible -i hosts all -m ping


Ansible Documention

For all the latest Ansible documentation always reference

https://docs.ansible.com/


Ansible Tower / AAP

This was the GUI front end to Ansible Engine that enables you to delegate tasks to users and it also provides logging, this has now been replaced with Ansible Automation Platform which is actually a suite of tools which allows you to automate your entire infrastructure with ease.


  



I created 3 ansible hosts in an ansible lab environment - using 3 virtual ubuntu machines on virt-manager (KVM) on asus laptop.

Asus laptop is the control host

ubuntu101
ubuntu102
ubuntu103

which are 

    192.168.122.101
    192.168.122.102
    192.168.122.103



then created an ansible hosts file on asus under /etc/ansible/hosts


root@asus:/etc/ansible# ansible all --list-hosts
  hosts (3):
    192.168.122.101
    192.168.122.102
    192.168.122.103
root@asus:/etc/ansible# 



next Set up SSH connections so Ansible can connect to the managed nodes.

Add your public SSH key to the authorized_keys file on each remote system.

 


I am using root in this case, so we copy paste the id_rsa.pub from asus /root/.ssh/id_rsa.pub to   each machine  into /root/.ssh/authorized_keys


then we can login without password for root:


root@asus:~/.ssh# ssh root@ubuntu101




do this on all 3 machines - done.



next, Ping the managed nodes.

ansible all -m ping



root@asus:~/.ssh# ansible all -m ping
192.168.122.103 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
192.168.122.102 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
192.168.122.101 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
root@asus:~/.ssh# 



NOTE: you must first ssh login using the IP address and not the hostname, otherwise this test will fail!!

eg

192.168.122.102 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Host key verification failed.",
    "unreachable": true
}


so make sure you first do a 

ssh root@192.168.122.102   instead of root@ubuntu102



Next create an inventory file on the control host:

under /home/kevin/LOCAL/WORK/AnsibleLab      (so it gets synced to my GitHab repo)

virtualmachines:
  hosts:
    ubuntu101:
      ansible_host: 192.168.122.101
    ubuntu102:
      ansible_host: 192.168.122.102
    ubuntu103:
      ansible_host: 192.168.122.103




next,

Verify your inventory. If you created your inventory in a directory other than your home directory, specify the full path with the -i option.

ansible-inventory -i inventory.yaml --list



kevin@asus:~/LOCAL/WORK/AnsibleLab$ ansible-inventory -i inventory.yaml --list
{
    "_meta": {
        "hostvars": {
            "ubuntu101": {
                "ansible_host": "192.168.122.101"
            },
            "ubuntu102": {
                "ansible_host": "192.168.122.102"
            },
            "ubuntu103": {
                "ansible_host": "192.168.122.103"
            }
        }
    },
    "all": {
        "children": [
            "ungrouped",
            "virtualmachines"
        ]
    },
    "virtualmachines": {
        "hosts": [
            "ubuntu101",
            "ubuntu102",
            "ubuntu103"
        ]
    }
}
kevin@asus:~/LOCAL/WORK/AnsibleLab$ 



then ping the managed nodes in your inventory.

In this example, the group name is virtualmachines which you can specify with the ansible command instead of all.

ansible virtualmachines -m ping -i inventory.yaml



 
 kevin@asus:~/LOCAL/WORK/AnsibleLab$ ansible virtualmachines -m ping  -i inventory.yaml
The authenticity of host '192.168.122.102 (192.168.122.102)' can't be established.
ED25519 key fingerprint is SHA256:vK5DgpWSrkMChzW18QCfI3NqhI0VzZcL/4+xI7pZTI0.
This host key is known by the following other names/addresses:
    ~/.ssh/known_hosts:35: ubuntu1
    ~/.ssh/known_hosts:38: ubuntu-base
    ~/.ssh/known_hosts:39: 192.168.122.101
    ~/.ssh/known_hosts:40: 192.168.122.100
    ~/.ssh/known_hosts:41: ubuntu102
    ~/.ssh/known_hosts:42: ubuntu103
    ~/.ssh/known_hosts:43: ubuntu101
The authenticity of host '192.168.122.103 (192.168.122.103)' can't be established.
ED25519 key fingerprint is SHA256:vK5DgpWSrkMChzW18QCfI3NqhI0VzZcL/4+xI7pZTI0.
This host key is known by the following other names/addresses:
    ~/.ssh/known_hosts:35: ubuntu1
    ~/.ssh/known_hosts:38: ubuntu-base
    ~/.ssh/known_hosts:39: 192.168.122.101
    ~/.ssh/known_hosts:40: 192.168.122.100
    ~/.ssh/known_hosts:41: ubuntu102
    ~/.ssh/known_hosts:42: ubuntu103
    ~/.ssh/known_hosts:43: ubuntu101
Are you sure you want to continue connecting (yes/no/[fingerprint])? ubuntu101 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: kevin@192.168.122.101: Permission denied (publickey,password,keyboard-interactive).",
    "unreachable": true
}

ubuntu102 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Host key verification failed.",
    "unreachable": true
}

ubuntu103 | UNREACHABLE! => {
    "changed": false,
    "msg": "Failed to connect to the host via ssh: Host key verification failed.",
    "unreachable": true
}
kevin@asus:~/LOCAL/WORK/AnsibleLab$ 


REASON FOR THE ABOVE ERRORS:

I had switched out of root and back to user kevin for ssh ... and this means my ssh keys arent corresponding to what is on the virtual machines on my lab.
#

below is how the output looks as root user:



root@asus:/home/kevin/LOCAL/WORK/AnsibleLab# ansible virtualmachines -m ping -i inventory.yaml
ubuntu102 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
ubuntu101 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
ubuntu103 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
root@asus:/home/kevin/LOCAL/WORK/AnsibleLab# 



Congratulations! You have successfully built an inventory.



Tips for building inventories 
Ensure that group names are meaningful and unique. Group names are also case sensitive.

Avoid spaces, hyphens, and preceding numbers (use floor_19, not 19th_floor) in group names.

Group hosts in your inventory logically according to their What, Where, and When.

What
Group hosts according to the topology, for example: db, web, leaf, spine.

Where
Group hosts by geographic location, for example: datacenter, region, floor, building.

When
Group hosts by stage, for example: development, test, staging, production.


TIP: 
Use metagroups 
Create a metagroup that organizes multiple groups in your inventory with the following syntax:

metagroupname:
  children:




also, Create variables

Variables set values for managed nodes, such as the IP address, FQDN, operating system, and SSH user, so you do not need to pass them when running Ansible commands.

Variables can apply to specific hosts.

webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
Variables can also apply to all hosts in a group.

webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
  vars:
    ansible_user: my_server_user

  

to add to git:

cd root@asus:/home/kevin/LOCAL/WORK/AnsibleLab# 


  git add inventory.yaml 
  git commit -m "added inventory.yaml"
  git branch -M main
  git remote add origin https://github.com/GHBeeGeePee/AnsibleLab.git
  
  git push -u origin main
 
  


14. How do you use Ansible to handle different operating systems and versions?
Ansible can handle different operating systems and versions by using different modules and conditional statements, here are a few ways to do it:

Use the ansible_os_family variable: Ansible automatically sets the ansible_os_family variable which can be used to check the family of the target operating system. For example, you can use this variable in a conditional statement to run specific tasks for different operating system families, such as Debian or Red Hat.
Use the ansible_distribution variable: Ansible automatically sets the ansible_distribution variable which can be used to check the specific distribution of the target operating system. For example, you can use this variable in a conditional statement to run specific tasks for different distributions, such as Ubuntu or Fedora.
Use the ansible_distribution_version variable: Ansible automatically sets the ansible_distribution_version variable which can be used to check the specific version of the target operating system. For example, you can use this variable in a conditional statement to run specific tasks for different versions of the same distribution, such as Ubuntu 18.04 or Ubuntu 20.04
Use the when statement: You can use the when statement to conditionally execute tasks based on the operating system and version. For example, you can use the when statement to run a task only on Ubuntu 18.04 and not on other versions.
Use different modules: Some modules are specific to certain operating systems or versions. For example, the apt module is specific to Debian-based systems, while the yum module is specific to Red Hat-based systems.
Use the setup module: The setup module can be used to gather information about the target system, such as the operating system and version. You can use this information in conjunction with the when statement to conditionally execute tasks based on the operating system and version.




 








